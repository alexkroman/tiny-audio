<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Class 4: Training | Tiny Audio Course</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reset.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/theme/black.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/monokai.css">
    <style>
        .reveal h1, .reveal h2, .reveal h3 { text-transform: none; }
        .reveal section img { border: none; background: none; box-shadow: none; }
        .reveal pre code { max-height: 500px; }
        .two-column { display: grid; grid-template-columns: 1fr 1fr; gap: 2em; align-items: start; }
        .highlight { color: #42affa; }
        .small { font-size: 0.7em; }
        .reveal ul { margin-top: 0.5em; }
        .reveal table { font-size: 0.8em; margin-top: 1em; }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <!-- Title Slide -->
            <section>
                <h1>Class 4</h1>
                <h2>Training</h2>
                <p>Parameter-efficient training and starting your first run</p>
                <p class="small">Tiny Audio Course • 20 minutes</p>
            </section>

            <!-- The Training Marathon -->
            <section>
                <h2>The Training Marathon</h2>
                <p>Training isn't a sprint; it's a marathon</p>
                <ul style="margin-top: 1.5em;">
                    <li><span class="highlight">Preparation:</span> Setup, data, and configuration</li>
                    <li><span class="highlight">Monitoring:</span> Keep an eye on your run</li>
                    <li><span class="highlight">Debugging:</span> Be ready for the unexpected</li>
                    <li><span class="highlight">Patience:</span> It takes time!</li>
                </ul>
            </section>

            <!-- The Full Fine-Tuning Problem -->
            <section>
                <h2>The Full Fine-Tuning Problem</h2>
                <p><strong>Traditional approach:</strong> Update all model parameters</p>
                <ul>
                    <li>HuBERT encoder: 1.3B params</li>
                    <li>Qwen-3 8B decoder: 8B params</li>
                    <li><strong>Total:</strong> 9.3B+ parameters to train</li>
                </ul>
                <p style="margin-top: 1.5em;"><span class="highlight">Problems:</span></p>
                <ul class="small">
                    <li>Requires massive GPU memory (80GB+ per GPU)</li>
                    <li>Very slow (weeks of training)</li>
                    <li>Expensive ($1000s in compute)</li>
                    <li>Risk of catastrophic forgetting</li>
                </ul>
            </section>

            <!-- The Solution: PEFT -->
            <section>
                <h2>The Solution: Parameter-Efficient Fine-Tuning</h2>
                <p><strong>Key insight:</strong> You don't need to update all parameters!</p>
                <p style="margin-top: 1.5em;">Instead, we:</p>
                <ol>
                    <li><strong>Freeze</strong> most parameters (keep pre-trained knowledge)</li>
                    <li><strong>Add small adapters</strong> that learn the specific task</li>
                    <li><strong>Train only adapters</strong> (~3% of total params)</li>
                </ol>
            </section>

            <!-- PEFT Results -->
            <section>
                <h2>PEFT Results</h2>
                <div class="two-column">
                    <div>
                        <h4 class="highlight">Traditional</h4>
                        <ul class="small">
                            <li>Training time: Weeks</li>
                            <li>Cost: $1000s</li>
                            <li>Memory: 80GB+</li>
                            <li>Risk: High forgetting</li>
                        </ul>
                    </div>
                    <div>
                        <h4 class="highlight">PEFT (LoRA)</h4>
                        <ul class="small">
                            <li>Training time: 24 hours</li>
                            <li>Cost: ~$12</li>
                            <li>Memory: 40GB</li>
                            <li>Risk: Low forgetting</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- What is LoRA -->
            <section>
                <h2>What is LoRA?</h2>
                <p><strong>LoRA</strong> = <strong>Lo</strong>w-<strong>R</strong>ank <strong>A</strong>daptation</p>
                <p style="margin-top: 1.5em;"><span class="highlight">Core idea:</span> Large weight matrices can be approximated by low-rank decompositions</p>
            </section>

            <!-- LoRA Math -->
            <section>
                <h2>LoRA: The Math (Simplified)</h2>
                <p>Normal training updates weight matrix W:</p>
                <p style="font-family: monospace; font-size: 0.9em;">W_new = W_old + ΔW</p>
                <p style="margin-top: 1.5em;">LoRA approximates ΔW with two small matrices:</p>
                <p style="font-family: monospace; font-size: 0.9em;">ΔW ≈ B × A</p>
                <ul style="margin-top: 1.5em;" class="small">
                    <li><strong>W is large:</strong> 2048 × 2048 = 4.2M params</li>
                    <li><strong>B is tall and thin:</strong> 2048 × 8 = 16K params</li>
                    <li><strong>A is short and wide:</strong> 8 × 2048 = 16K params</li>
                    <li><strong class="highlight">Total:</strong> 32K params instead of 4.2M! (0.76%)</li>
                </ul>
            </section>

            <!-- LoRA Rank -->
            <section>
                <h2>LoRA Rank (r)</h2>
                <p><strong>Rank (r):</strong> The middle dimension</p>
                <ul>
                    <li>Lower rank = fewer parameters, less capacity</li>
                    <li>Higher rank = more parameters, more capacity</li>
                </ul>
                <p style="margin-top: 1.5em;"><span class="highlight">Tiny Audio:</span></p>
                <ul>
                    <li><strong>Encoder:</strong> r=8 (conservative, ~2M params)</li>
                    <li><strong>Decoder:</strong> r=64 (more capacity, ~15M params)</li>
                </ul>
            </section>

            <!-- How LoRA Works -->
            <section>
                <h2>How LoRA Works in Practice</h2>
                <pre style="font-size: 0.7em;"><code class="language-python" data-trim>
# Original forward pass
output = linear_layer(input)  # Uses W

# With LoRA
output = linear_layer(input) + lora_B(lora_A(input))
         #--- frozen ----#   #--- trainable ---#
                </code></pre>
                <ul style="margin-top: 1em;">
                    <li><strong>During training:</strong> W stays frozen, only B and A get updates</li>
                    <li><strong>During inference:</strong> Can merge W' = W + B×A (no speed penalty!)</li>
                </ul>
            </section>

            <!-- LoRA Hyperparameters -->
            <section>
                <h2>LoRA Hyperparameters</h2>
                <ul>
                    <li><strong>Rank (r):</strong> Controls adapter capacity
                        <ul class="small">
                            <li>Encoder: r=8, Decoder: r=64</li>
                        </ul>
                    </li>
                    <li><strong>Alpha (lora_alpha):</strong> Scaling factor
                        <ul class="small">
                            <li>scale = alpha / r</li>
                            <li>Encoder: alpha=8 (scale=1.0)</li>
                            <li>Decoder: alpha=32 (scale=0.5)</li>
                        </ul>
                    </li>
                    <li><strong>Target Modules:</strong> Which layers get adapters
                        <ul class="small">
                            <li>Encoder: q_proj, k_proj</li>
                            <li>Decoder: q_proj, v_proj</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <!-- Why These Configurations -->
            <section>
                <h2>Why These Specific Configurations?</h2>
                <ul>
                    <li><strong>Encoder (r=8, small):</strong>
                        <ul class="small">
                            <li>Already well pre-trained on speech</li>
                            <li>Just needs small adjustments (~2M params)</li>
                        </ul>
                    </li>
                    <li><strong>Decoder (r=64, larger):</strong>
                        <ul class="small">
                            <li>Bigger adaptation needed (text → speech-aware text)</li>
                            <li>More capacity for language generation (~15M params)</li>
                        </ul>
                    </li>
                    <li><strong>Projector (no LoRA):</strong>
                        <ul class="small">
                            <li>Brand new component (no pre-training)</li>
                            <li>Train fully from scratch (~122M params)</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <!-- What is Hydra -->
            <section>
                <h2>Training Configuration with Hydra</h2>
                <p><strong>Hydra:</strong> Configuration management framework</p>
                <ul>
                    <li>Compose configs from multiple files</li>
                    <li>Override via command line</li>
                    <li>Experiment tracking</li>
                    <li>Clean, maintainable configs</li>
                </ul>
            </section>

            <!-- Config Structure -->
            <section>
                <h2>Tiny Audio Config Structure</h2>
                <pre style="font-size: 0.6em;"><code data-trim>
configs/hydra/
├── config.yaml              # Main config
├── model/                   # Model architecture
├── data/                    # Dataset config
├── training/                # Training hyperparameters
├── peft/                    # Decoder LoRA config
├── encoder_lora/            # Encoder LoRA config
└── experiments/             # Full experiment presets
    ├── stage1.yaml          # Full training
    └── mac_minimal.yaml     # Local testing
                </code></pre>
            </section>

            <!-- Key Hyperparameters -->
            <section>
                <h2>Key Training Hyperparameters</h2>
                <ul class="small">
                    <li><strong>Learning Rate:</strong> 1e-4 (how fast the model learns)</li>
                    <li><strong>Learning Rate Schedule:</strong> Cosine decay with warmup</li>
                    <li><strong>Batch Size:</strong> 8 per device (samples per gradient update)</li>
                    <li><strong>Gradient Accumulation:</strong> 4 steps (effective batch size = 32)</li>
                    <li><strong>Max Steps:</strong> 10,000 (~24 hours on A40)</li>
                    <li><strong>Mixed Precision:</strong> bf16 (for speed and memory)</li>
                </ul>
            </section>

            <!-- Pre-flight Checklist -->
            <section>
                <h2>Pre-flight Checklist</h2>
                <p>Before a long training run, always check:</p>
                <ul>
                    <li><span class="highlight">Infrastructure:</span> Is the GPU working?</li>
                    <li><span class="highlight">Evaluation:</span> Are your evals ready?</li>
                    <li><span class="highlight">Checkpoints:</span> Is auto-resume working?</li>
                    <li><span class="highlight">Logging:</span> Are you logging all key metrics?</li>
                </ul>
            </section>

            <!-- Scaling Laws -->
            <section>
                <h2>A Glimpse into Scaling Laws</h2>
                <p>How do big labs decide what to train?</p>
                <p style="margin-top: 1.5em;">They use <strong class="highlight">scaling laws</strong> to predict performance based on:</p>
                <ul>
                    <li>Model size (parameters)</li>
                    <li>Training data (tokens)</li>
                    <li>Compute (FLOPs)</li>
                </ul>
                <p class="small" style="margin-top: 1em;">This helps them allocate their massive compute budgets effectively.</p>
            </section>

            <!-- Key Takeaways -->
            <section>
                <h2>Key Takeaways</h2>
                <ul>
                    <li>PEFT trains only ~3% of parameters (139M / 4.3B)</li>
                    <li>LoRA approximates weight updates with low-rank matrices</li>
                    <li>Encoder uses r=8, Decoder uses r=64</li>
                    <li>Hydra manages training configuration</li>
                    <li>Training takes ~24 hours for ~$12 on A40</li>
                    <li>No speed penalty during inference (adapters can merge)</li>
                </ul>
            </section>

            <!-- Today's Workshop -->
            <section>
                <h2>Today's Hands-On Workshop</h2>
                <p>What you'll do in the next 40 minutes:</p>
                <ul>
                    <li><strong>Exercise 1:</strong> Explore training configs (Hydra structure)</li>
                    <li><strong>Exercise 2:</strong> Set up cloud GPU (RunPod or similar)</li>
                    <li><strong>Exercise 3:</strong> Start your first training run</li>
                    <li><strong>Exercise 4:</strong> Monitor training progress (wandb)</li>
                </ul>
                <p style="margin-top: 1.5em; color: #42affa;">
                    By the end, you'll have a model training in the cloud!
                </p>
            </section>

            <!-- Thank You -->
            <section>
                <h2>Questions?</h2>
                <p>Let's move to the hands-on workshop!</p>
                <p class="small">Press <code>Esc</code> for slide overview • <code>S</code> for speaker notes</p>
            </section>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/highlight.js"></script>
    <script>
        Reveal.initialize({
            hash: true,
            plugins: [ RevealNotes, RevealHighlight ],
            transition: 'slide',
            backgroundTransition: 'fade'
        });
    </script>
</body>
</html>
