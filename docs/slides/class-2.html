<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Class 2: Audio Processing and Encoders | Tiny Audio Course</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reset.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/theme/black.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/monokai.css">
    <style>
        .reveal h1, .reveal h2, .reveal h3 { text-transform: none; }
        .reveal section img { border: none; background: none; box-shadow: none; }
        .reveal pre code { max-height: 500px; }
        .two-column { display: grid; grid-template-columns: 1fr 1fr; gap: 2em; align-items: start; }
        .highlight { color: #42affa; }
        .small { font-size: 0.7em; }
        .reveal ul { margin-top: 0.5em; }
        .reveal table { font-size: 0.8em; margin-top: 1em; }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <!-- Title Slide -->
            <section>
                <h1>Class 2</h1>
                <h2>Audio Processing and Encoders</h2>
                <p>How audio becomes data and how HuBERT processes it</p>
                <p class="small">Tiny Audio Course ‚Ä¢ 20 minutes</p>
            </section>

            <!-- Data Quality -->
            <section>
                <h2>The Importance of Data Quality</h2>
                <p>The single most important factor in training a great model</p>
                <ul style="margin-top: 1.5em;">
                    <li><span class="highlight">Good Data:</span> A clear, consistent signal for the model to learn from</li>
                    <li><span class="highlight">Bad Data:</span> Noise that confuses the model and hurts performance</li>
                </ul>
                <p style="margin-top: 1.5em;">Our goal: Turn raw, messy audio into good data.</p>
            </section>

            <!-- What is Sound -->
            <section>
                <h2>What is Sound?</h2>
                <p>Sound = vibrations traveling through air as pressure waves</p>
                <div style="margin-top: 1.5em;">
                    <p><span class="highlight">Key Properties:</span></p>
                    <ul>
                        <li><strong>Amplitude:</strong> How loud (wave height)</li>
                        <li><strong>Frequency:</strong> Pitch (oscillation speed)</li>
                        <li><strong>Duration:</strong> How long</li>
                    </ul>
                </div>
            </section>

            <!-- Digitizing Audio -->
            <section>
                <h2>Digitizing Audio</h2>
                <p>Computers need numbers, not waves. We use <strong>sampling</strong></p>
                <ul>
                    <li>CD quality: 44,100 Hz</li>
                    <li>Phone: 8,000 Hz</li>
                    <li><strong class="highlight">Tiny Audio: 16,000 Hz</strong> ‚Üê Perfect for speech!</li>
                </ul>
                <p style="margin-top: 1.5em;"><span class="highlight">Example:</span></p>
                <ul class="small">
                    <li>1 second of audio at 16kHz = 16,000 numbers</li>
                    <li>3 second audio file = 48,000 numbers!</li>
                </ul>
            </section>

            <!-- Why 16 kHz -->
            <section>
                <h2>Why 16 kHz?</h2>
                <ul>
                    <li>Captures human speech frequency range (85-255 Hz fundamental + harmonics to ~8 kHz)</li>
                    <li>Balances quality vs efficiency</li>
                    <li>Industry standard for ASR</li>
                </ul>
                <p class="small highlight" style="margin-top: 1.5em;">
                    üß™ <strong>Today's Experiment:</strong> Test how different sample rates (8kHz vs 16kHz vs 44.1kHz) affect quality
                </p>
            </section>

            <!-- Feature Extraction Problem -->
            <section>
                <h2>Feature Extraction: The Problem</h2>
                <p>Raw waveforms are challenging:</p>
                <ul>
                    <li>High-dimensional (16,000 numbers/second!)</li>
                    <li>Noisy</li>
                    <li>Hard for models to learn from</li>
                </ul>
            </section>

            <!-- Feature Extraction Solution -->
            <section>
                <h2>Feature Extraction: The Solution</h2>
                <p><strong>Wav2Vec2FeatureExtractor</strong> transforms audio through:</p>
                <ol>
                    <li><strong>Resampling:</strong> Convert any sample rate ‚Üí 16 kHz</li>
                    <li><strong>Z-Normalization:</strong> (x - mean) / std
                        <ul class="small">
                            <li>Centers audio around 0</li>
                            <li>Scales to unit variance</li>
                            <li>Stabilizes training</li>
                        </ul>
                    </li>
                    <li><strong>Padding:</strong> Makes all samples same length (enables batching)</li>
                    <li><strong>Tensor Conversion:</strong> NumPy arrays ‚Üí PyTorch tensors</li>
                </ol>
            </section>

            <!-- What is HuBERT -->
            <section>
                <h2>What is HuBERT?</h2>
                <p><strong>HuBERT</strong> = <strong>H</strong>idden <strong>U</strong>nit <strong>BERT</strong></p>
                <p style="margin-top: 1.5em;"><span class="highlight">Key innovation:</span> Self-supervised learning as data curation at scale</p>
                <ul>
                    <li>Leverages massive, diverse datasets to build foundational knowledge</li>
                    <li>Trained on 60,000 hours of unlabeled speech</li>
                    <li>Learns by predicting masked audio segments</li>
                </ul>
                <p class="small" style="margin-top: 1.5em;">üéµ <em>Analogy: Like learning a language by listening to thousands of hours of conversation and learning to predict missing words, without ever opening a dictionary.</em></p>
            </section>

            <!-- HuBERT Architecture -->
            <section>
                <h2>HuBERT Architecture</h2>
                <ul style="font-family: monospace; font-size: 0.8em; list-style: none; margin-left: 0;">
                    <li>Audio waveform (16 kHz)</li>
                    <li>&nbsp;&nbsp;‚Üì</li>
                    <li>CNN Feature Encoder (7 conv layers)</li>
                    <li>&nbsp;&nbsp;‚Üì (~320x time compression)</li>
                    <li>Audio features (~50 Hz)</li>
                    <li>&nbsp;&nbsp;‚Üì</li>
                    <li>24 Transformer Layers</li>
                    <li>&nbsp;&nbsp;‚Üì</li>
                    <li>1280-dim embeddings per frame</li>
                </ul>
            </section>

            <!-- HuBERT Stats -->
            <section>
                <h2>HuBERT-XLarge Stats</h2>
                <ul>
                    <li>24 transformer layers</li>
                    <li>1280 hidden dimensions</li>
                    <li>16 attention heads/layer</li>
                    <li>~1.3 billion parameters</li>
                    <li>Pre-trained on LibriLight (60K hours)</li>
                </ul>
            </section>

            <!-- What HuBERT Learned -->
            <section>
                <h2>What HuBERT Learned</h2>
                <p>During pre-training, HuBERT learned:</p>
                <ul>
                    <li>Phonemes (speech sounds)</li>
                    <li>Speaker characteristics</li>
                    <li>Acoustic environments</li>
                    <li>Prosody (rhythm, stress, intonation)</li>
                </ul>
                <p style="margin-top: 1.5em; color: #42affa;">
                    <strong>This is why we don't train from scratch!</strong>
                </p>
            </section>

            <!-- Time Compression -->
            <section>
                <h2>Time Compression</h2>
                <p style="font-family: monospace; font-size: 0.8em;">
                    3 seconds audio at 16kHz = 48,000 samples<br>
                    &nbsp;&nbsp;‚Üì (HuBERT encoder)<br>
                    ~149 embeddings √ó 1280 dimensions
                </p>
                <ul style="margin-top: 1.5em;">
                    <li><strong>Compression:</strong> ~320x reduction</li>
                    <li><strong>Each embedding:</strong> Represents ~20ms of audio</li>
                    <li><strong>Dense representation:</strong> More meaningful than raw waveform</li>
                </ul>
            </section>

            <!-- LoRA Adaptation -->
            <section>
                <h2>LoRA Adaptation</h2>
                <p>In Tiny Audio, we add small LoRA adapters:</p>
                <ul>
                    <li><strong>Base model:</strong> Frozen (1.3B params)</li>
                    <li><strong>LoRA adapters:</strong> Trainable (~2M params, r=8)</li>
                    <li><strong>Target:</strong> q_proj, k_proj in attention layers</li>
                    <li><strong>Result:</strong> 0.15% of encoder params are trainable!</li>
                </ul>
                <p class="small" style="margin-top: 1.5em;">üì∏ <em>Analogy: Putting adjustable glasses on a camera - camera unchanged, but output is tuned</em></p>
            </section>

            <!-- Workshop Experiments -->
            <section>
                <h2>Workshop Experiments</h2>
                <p>What you'll explore today:</p>
                <ul>
                    <li>üéµ Compare sample rates and their effects</li>
                    <li>üîä Test noise robustness with different levels</li>
                    <li>üî¨ Compare HuBERT vs Wav2Vec2 encoders</li>
                    <li>üìä Visualize embedding patterns</li>
                    <li>‚öôÔ∏è Experiment with LoRA ranks (4, 8, 16, 32)</li>
                    <li>üìà Measure parameter counts and memory usage</li>
                </ul>
                <p class="small highlight" style="margin-top: 1em;">
                    Each experiment builds understanding of the encoder's role
                </p>
            </section>

            <!-- Key Takeaways -->
            <section>
                <h2>Key Takeaways</h2>
                <ul>
                    <li>Audio is digitized through sampling (16kHz for speech)</li>
                    <li>Wav2Vec2FeatureExtractor normalizes audio for training</li>
                    <li>HuBERT compresses audio ~320x (48k samples ‚Üí 149 embeddings)</li>
                    <li>Each embedding represents ~20ms of audio in 1280 dimensions</li>
                    <li>Only 0.15% of encoder parameters are trainable with LoRA</li>
                </ul>
            </section>

            <!-- Today's Workshop -->
            <section>
                <h2>Today's Hands-On Workshop</h2>
                <p>What you'll do in the next 40 minutes:</p>
                <ul>
                    <li><strong>Exercise 1:</strong> Visualize audio processing (raw vs normalized)</li>
                    <li><strong>Exercise 2:</strong> Explore HuBERT outputs (dimensions and compression)</li>
                    <li><strong>Exercise 3:</strong> Count trainable parameters by component</li>
                </ul>
                <p style="margin-top: 1.5em; color: #42affa;">
                    By the end, you'll see exactly how audio becomes embeddings!
                </p>
            </section>

            <!-- Thank You -->
            <section>
                <h2>Questions?</h2>
                <p>Let's move to the hands-on workshop!</p>
                <p class="small">Press <code>Esc</code> for slide overview ‚Ä¢ <code>S</code> for speaker notes</p>
            </section>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/highlight.js"></script>
    <script>
        Reveal.initialize({
            hash: true,
            plugins: [ RevealNotes, RevealHighlight ],
            transition: 'slide',
            backgroundTransition: 'fade'
        });
    </script>
</body>
</html>
