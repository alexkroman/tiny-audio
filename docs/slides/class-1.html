<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Class 1: Introduction and Setup | Tiny Audio Course</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reset.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/theme/black.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/monokai.css">
    <style>
        /* Minimal custom styling - let reveal.js handle most */
        .reveal h1, .reveal h2, .reveal h3 { text-transform: none; }
        .two-column { display: grid; grid-template-columns: 1fr 1fr; gap: 2em; align-items: start; }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <!-- Title Slide -->
            <section>
                <h1>Class 1</h1>
                <h2>Introduction and Setup</h2>
                <p>Understanding ASR and the Tiny Audio Architecture</p>
            </section>

            <!-- Why is ASR Hard -->
            <section>
                <h3>Why is ASR Hard?</h3>
                <div class="two-column">
                    <div>
                        <h4>Audio Variability</h4>
                        <ul>
                            <li>Different accents</li>
                            <li>Speaking speeds</li>
                            <li>Background noise</li>
                        </ul>
                    </div>
                    <div>
                        <h4>Linguistic Challenges</h4>
                        <ul>
                            <li>Ambiguity ("ice cream" vs "I scream")</li>
                            <li>Context dependency</li>
                            <li>Real-time constraints</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- Evolution of ASR -->
            <section>
                <h3>How Modern ASR Works</h3>
                <table style="font-size: 0.65em;">
                    <tr>
                        <th>Generation</th>
                        <th>Era</th>
                        <th>Technology</th>
                        <th>Key Features</th>
                    </tr>
                    <tr>
                        <td>1st</td>
                        <td>1950s-1980s</td>
                        <td>Rule-based</td>
                        <td>~100 words, speaker-dependent</td>
                    </tr>
                    <tr>
                        <td>2nd</td>
                        <td>1980s-2010s</td>
                        <td>Hidden Markov Models</td>
                        <td>Better accuracy, struggled with noise</td>
                    </tr>
                    <tr>
                        <td>3rd</td>
                        <td>2010s-2020s</td>
                        <td>Deep Learning</td>
                        <td>RNNs/LSTMs ‚Üí Attention ‚Üí Transformers</td>
                    </tr>
                    <tr>
                        <td><strong>4th</strong></td>
                        <td><strong>2020s-Present</strong></td>
                        <td><strong>Self-Supervised + LLMs</strong></td>
                        <td><strong>wav2vec, HuBERT, Whisper ‚Üê Tiny Audio!</strong></td>
                    </tr>
                </table>
            </section>

            <!-- Architecture Overview -->
            <section>
                <h2>The Tiny Audio Architecture</h2>
                <p>Three components working together</p>
                <ul>
                    <li><strong>Audio Encoder</strong> (Whisper) - Converts audio to embeddings</li>
                    <li><strong>Audio Projector</strong> (SwiGLU MLP) - Bridges audio ‚Üî language</li>
                    <li><strong>Language Model</strong> (SmollM3) - Generates text transcription</li>
                </ul>
                <p>
                    <small>Throughout the course, you'll be able to experiment with switching these components</small>
                </p>
            </section>

            <!-- Component 1: Audio Encoder -->
            <section>
                <h3>Component 1: Audio Encoder</h3>
                <p><strong>Whisper</strong> (1.55 billion parameters)</p>
                <ul>
                    <li><strong>Purpose:</strong> Convert raw audio ‚Üí meaningful features</li>
                    <li><strong>Input:</strong> Waveform (16,000 numbers/second)</li>
                    <li><strong>Output:</strong> Embeddings (one per ~20ms of audio)</li>
                    <li><strong>Key insight:</strong> Pre-trained on massive dataset of diverse audio!</li>
                </ul>
                <p><small>üéµ <em>Analogy: An expert musician who can listen to any piece and transcribe it</em></small></p>
            </section>

            <!-- Component 2: Audio Projector -->
            <section>
                <h3>Component 2: Audio Projector</h3>
                <p><strong>SwiGLU MLP</strong> (~50 million parameters)</p>
                <ul>
                    <li><strong>Purpose:</strong> Bridge the gap between audio and language worlds</li>
                    <li><strong>Process:</strong> Downsamples by 5x + transforms dimensions</li>
                    <li><strong>Architecture:</strong> Pre-norm + gated projection + post-norm</li>
                    <li><strong>Key insight:</strong> LARGEST trainable component - all ~50M parameters learn!</li>
                </ul>
                <p><small>üåâ <em>Analogy: A skilled diplomat fluently translating between two cultures</em></small></p>
            </section>

            <!-- Component 3: Language Model -->
            <section>
                <h3>Component 3: Language Model Decoder</h3>
                <p><strong>SmollM3</strong> (3 billion parameters)</p>
                <ul>
                    <li><strong>Purpose:</strong> Generate coherent, grammatically correct text</li>
                    <li><strong>Input:</strong> Audio embeddings (via projector)</li>
                    <li><strong>Output:</strong> Text with grammar, spelling, punctuation</li>
                    <li><strong>Key insight:</strong> Pre-trained on large amounts of text!</li>
                    <li><strong>Training:</strong> Uses LoRA for efficient adaptation</li>
                </ul>
                <p><small>‚úçÔ∏è <em>Analogy: A master storyteller weaving events into a compelling narrative</em></small></p>
            </section>

            <!-- Why This Architecture -->
            <section>
                <h3>Why This Architecture?</h3>
                <div class="two-column">
                    <div>
                        <h4>Efficiency</h4>
                        <p>Train only <strong>~58M params</strong><br>(~1.3% of 4.6B total)</p>
                        <ul>
                            <li>Projector: ~50M</li>
                            <li>Encoder LoRA: ~4M</li>
                            <li>Decoder LoRA: ~4M</li>
                        </ul>
                    </div>
                    <div>
                        <h4>Cost & Speed</h4>
                        <p>Training completes in:</p>
                        <ul>
                            <li>‚è±Ô∏è ~24 hours</li>
                            <li>üí∞ ~$12 total cost</li>
                            <li>üñ•Ô∏è Single GPU</li>
                            <li>‚úÖ Leverages pre-trained knowledge</li>
                        </ul>
                    </div>
                </div>
                <p><small><strong>Note:</strong> We use instruct-tuned models (SmollM3 Instruct) which perform better for transcription than base models. Dense transformers are stable and well-understood - perfect for our focused ASR task.</small></p>
            </section>

            <!-- Course Goals -->
            <section>
                <h2>Course Goals</h2>
                <p>What you'll build in 6 hours</p>
                <ul>
                    <li>‚úÖ Train your own customized ASR model</li>
                    <li>‚úÖ Evaluate it on standard benchmarks</li>
                    <li>‚úÖ Push it to your HuggingFace account</li>
                    <li>‚úÖ Add results to the community leaderboard</li>
                </ul>
                <p>
                    This isn't just learning‚Äî<br>you'll have a <strong>real, working, deployed model</strong> with your name on it!
                </p>
            </section>

            <!-- Course Structure -->
            <section>
                <h3>Course Structure</h3>
                <ol>
                    <li><strong>Class 1:</strong> Introduction and Setup (today!)</li>
                    <li><strong>Class 2:</strong> Audio Processing and Encoders</li>
                    <li><strong>Class 3:</strong> Language Models and Projectors</li>
                    <li><strong>Class 4:</strong> Training</li>
                    <li><strong>Class 5:</strong> Evaluation and Debugging</li>
                    <li><strong>Class 6:</strong> Publishing and Deployment</li>
                </ol>
                <p><small>Each class: 20 min lecture + 40 min hands-on workshop</small></p>
            </section>

            <!-- Experimentation Preview -->
            <section>
                <h2>Workshop Experiments</h2>
                <p>What you'll try today:</p>
                <ul>
                    <li>üé§ Launch Gradio demo with microphone & file upload</li>
                    <li>üéØ Try different tasks (transcribe, describe, emotion, custom)</li>
                    <li>üìä Run evaluation on LoquaciousSet (100 samples)</li>
                    <li>üìà Analyze Word Error Rate (WER) results</li>
                </ul>
            </section>

            <!-- Today's Workshop -->
            <section>
                <h2>Today's Hands-On Workshop</h2>
                <p>What you'll do in the next 40 minutes:</p>
                <ul>
                    <li><strong>Exercise 1:</strong> Set up your environment (install dependencies, download samples)</li>
                    <li><strong>Exercise 2:</strong> Launch Gradio demo and run evaluation</li>
                    <li><strong>Exercise 3:</strong> Explore the codebase</li>
                </ul>
                <p>
                    By the end, you'll have an interactive demo running and see quantitative benchmarks!
                </p>
            </section>

            <!-- Thank You -->
            <section>
                <h2>Questions?</h2>
                <p>Let's move to the hands-on workshop!</p>
                <p><small>Press <code>Esc</code> for slide overview ‚Ä¢ <code>S</code> for speaker notes</small></p>
            </section>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/highlight.js"></script>
    <script>
        Reveal.initialize({
            hash: true,
            plugins: [ RevealNotes, RevealHighlight ],
            transition: 'slide',
            backgroundTransition: 'fade'
        });
    </script>
</body>
</html>
