<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Class 5: Evaluation and Debugging | Tiny Audio Course</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reset.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/theme/black.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/monokai.css">
    <style>
        .reveal h1, .reveal h2, .reveal h3 { text-transform: none; }
        .reveal section img { border: none; background: none; box-shadow: none; }
        .reveal pre code { max-height: 500px; }
        .two-column { display: grid; grid-template-columns: 1fr 1fr; gap: 2em; align-items: start; }
        .highlight { color: #42affa; }
        .small { font-size: 0.7em; }
        .reveal ul { margin-top: 0.5em; }
        .reveal table { font-size: 0.8em; margin-top: 1em; }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <!-- Title Slide -->
            <section>
                <h1>Class 5</h1>
                <h2>Evaluation and Debugging</h2>
                <p>Measure performance and improve your model</p>
                <p class="small">Tiny Audio Course • 20 minutes</p>
            </section>

            <!-- A Philosophy of Evaluation -->
            <section>
                <h2>A Philosophy of Evaluation</h2>
                <p>A single metric is not enough</p>
                <ul style="margin-top: 1.5em;">
                    <li><span class="highlight">Primary Metric (WER):</span> Good for tracking progress</li>
                    <li><span class="highlight">Secondary Benchmarks:</span> Test for robustness, different accents, etc.</li>
                    <li><span class="highlight">Qualitative Analysis ("Vibe Testing"):</span> Interact with your model to find quirks</li>
                </ul>
            </section>

            <!-- What is WER -->
            <section>
                <h2>What is WER?</h2>
                <p><strong>WER</strong> = <strong>W</strong>ord <strong>E</strong>rror <strong>R</strong>ate</p>
                <p>The standard metric for evaluating ASR systems</p>
                <p style="font-family: monospace; font-size: 0.9em; margin-top: 1.5em;">
                    WER = (Substitutions + Insertions + Deletions) / Total Words
                </p>
            </section>

            <!-- WER Example -->
            <section>
                <h2>WER Example</h2>
                <pre style="font-size: 0.7em;"><code data-trim>
Reference:  "the quick brown fox jumps"  (5 words)
Hypothesis: "the quick brown dog jumped"

Errors:
- Substitution: "fox" → "dog"     (1 error)
- Substitution: "jumps" → "jumped" (1 error)

WER = 2 / 5 = 0.40 = 40%
                </code></pre>
            </section>

            <!-- Types of Errors -->
            <section>
                <h2>Types of Errors</h2>
                <ul>
                    <li><strong>Substitution (S):</strong> Wrong word
                        <ul class="small"><li>Ref: "the cat sat" → Hyp: "the dog sat"</li></ul>
                    </li>
                    <li><strong>Insertion (I):</strong> Extra word
                        <ul class="small"><li>Ref: "hello world" → Hyp: "hello big world"</li></ul>
                    </li>
                    <li><strong>Deletion (D):</strong> Missing word
                        <ul class="small"><li>Ref: "hello world" → Hyp: "hello"</li></ul>
                    </li>
                </ul>
            </section>

            <!-- What's a Good WER -->
            <section>
                <h2>What's a Good WER?</h2>
                <table>
                    <tr>
                        <th>WER Range</th>
                        <th>Quality</th>
                        <th>Description</th>
                    </tr>
                    <tr>
                        <td>&lt; 5%</td>
                        <td>Excellent</td>
                        <td>Commercial systems</td>
                    </tr>
                    <tr>
                        <td>5-10%</td>
                        <td>Very good</td>
                        <td>Usable for most tasks</td>
                    </tr>
                    <tr>
                        <td>10-20%</td>
                        <td>Good</td>
                        <td>Acceptable</td>
                    </tr>
                    <tr style="color: #42affa;">
                        <td>~12%</td>
                        <td>Tiny Audio</td>
                        <td>Our target!</td>
                    </tr>
                    <tr>
                        <td>20-30%</td>
                        <td>Fair</td>
                        <td>Needs improvement</td>
                    </tr>
                    <tr>
                        <td>&gt; 30%</td>
                        <td>Poor</td>
                        <td>Significant issues</td>
                    </tr>
                </table>
            </section>

            <!-- Context Matters -->
            <section>
                <h2>Context Matters</h2>
                <p>WER expectations vary by use case:</p>
                <ul>
                    <li><strong>Clean speech:</strong> Lower WER expected</li>
                    <li><strong>Noisy environments:</strong> Higher WER acceptable</li>
                    <li><strong>Accented speech:</strong> Often higher WER</li>
                    <li><strong>Domain-specific:</strong> Medical/legal need very low WER</li>
                </ul>
            </section>

            <!-- LoquaciousSet Benchmark -->
            <section>
                <h2>LoquaciousSet Benchmark</h2>
                <p><strong>Dataset:</strong> LoquaciousSet test split</p>
                <ul>
                    <li>Diverse speakers and accents</li>
                    <li>Various acoustic conditions</li>
                    <li>Multiple speech types</li>
                    <li>Industry-standard benchmark</li>
                </ul>
                <p style="margin-top: 1.5em;"><span class="highlight">Why this dataset?</span></p>
                <ul class="small">
                    <li>Representative of real-world speech</li>
                    <li>Fair comparison with other models</li>
                </ul>
            </section>

            <!-- Evaluation Process -->
            <section>
                <h2>Evaluation Process</h2>
                <ol class="small">
                    <li>Load trained model</li>
                    <li>Load test dataset samples</li>
                    <li>For each audio sample:
                        <ul>
                            <li>Run inference (get transcription)</li>
                            <li>Normalize reference and hypothesis</li>
                            <li>Calculate WER</li>
                        </ul>
                    </li>
                    <li>Aggregate results</li>
                    <li>Analyze errors</li>
                </ol>
            </section>

            <!-- Text Normalization -->
            <section>
                <h2>Text Normalization</h2>
                <p>Both reference and hypothesis are normalized:</p>
                <ol class="small">
                    <li>Remove <code>&lt;inaudible&gt;</code> tags</li>
                    <li>Remove disfluencies ("uh", "um")</li>
                    <li>Apply Whisper normalization
                        <ul>
                            <li>Lowercase</li>
                            <li>Remove punctuation</li>
                            <li>Expand contractions</li>
                        </ul>
                    </li>
                    <li>Apply truecasing</li>
                </ol>
                <p style="margin-top: 1em; color: #42affa;"><strong>Why?</strong> Fair comparison - "Hello" vs "hello" shouldn't count as error!</p>
            </section>

            <!-- Proactive Stability Measures -->
            <section>
                <h2>Proactive Stability Measures</h2>
                <p>Preventing instabilities before they happen</p>
                <ul>
                    <li><span class="highlight">Data Filtering & Shuffling:</span> Clean data is stable data</li>
                    <li><span class="highlight">Z-loss:</span> Regularization to prevent large logits</li>
                    <li><span class="highlight">QK-Norm:</span> Normalize query and key vectors in attention</li>
                </ul>
            </section>

            <!-- Debugging Common Issues -->
            <section>
                <h2>Debugging Common Issues</h2>
                <p>Training instabilities are a normal part of the process</p>
                <ul style="margin-top: 1.5em;">
                    <li><strong>Recoverable Spikes:</strong> Loss jumps but returns. Usually caused by bad data.</li>
                    <li><strong>Non-recoverable Spikes:</strong> Loss jumps and stays high. Requires intervention.</li>
                </ul>
            </section>

            <!-- Common Culprits -->
            <section>
                <h2>Common Culprits for Instability</h2>
                <ul>
                    <li><span class="highlight">Learning Rate Too High:</span> The most common cause</li>
                    <li><span class="highlight">Bad Data:</span> Corrupted or out-of-distribution samples</li>
                    <li><span class="highlight">Data-Parameter Interactions:</span> Bad luck!</li>
                </ul>
            </section>

            <!-- Damage Control -->
            <section>
                <h2>Damage Control</h2>
                <p>When a non-recoverable spike happens:</p>
                <ol>
                    <li><strong>Rewind and Skip:</strong> Go back to a previous checkpoint and skip the problematic batch.</li>
                    <li><strong>Reduce Learning Rate:</strong> If instability persists, lower the learning rate.</li>
                    <li><strong>Tighten Gradient Clipping:</strong> Reduce `max_grad_norm` to prevent large gradients.</li>
                </ol>
            </section>

            <!-- Training Issue 1 -->
            <section>
                <h2>Issue 1: Loss Not Decreasing</h2>
                <p><strong>Symptoms:</strong></p>
                <ul class="small">
                    <li>Loss stays flat or increases</li>
                    <li>No improvement over time</li>
                </ul>
                <p style="margin-top: 1em;"><strong>Possible causes:</strong></p>
                <ul class="small">
                    <li>Learning rate too high</li>
                    <li>Batch size too small</li>
                    <li>Data quality issues</li>
                </ul>
                <p style="margin-top: 1em;"><strong>Solutions:</strong></p>
                <ul class="small">
                    <li>Lower learning rate (5e-5 instead of 1e-4)</li>
                    <li>Increase gradient accumulation steps</li>
                </ul>
            </section>

            <!-- Training Issue 2 -->
            <section>
                <h2>Issue 2: Overfitting</h2>
                <p><strong>Symptoms:</strong></p>
                <ul class="small">
                    <li>Training loss goes down</li>
                    <li>Validation loss goes up</li>
                </ul>
                <p style="margin-top: 1em;"><strong>Solutions:</strong></p>
                <ul class="small">
                    <li>Add more training data</li>
                    <li>Increase regularization (LoRA dropout: 0.1)</li>
                    <li>Early stopping</li>
                </ul>
            </section>

            <!-- Training Issue 3 -->
            <section>
                <h2>Issue 3: Model Outputs Gibberish</h2>
                <p><strong>Symptoms:</strong></p>
                <ul class="small">
                    <li>Transcriptions are nonsense</li>
                    <li>Repeating words</li>
                    <li>Random characters</li>
                </ul>
                <p style="margin-top: 1em;"><strong>Possible causes:</strong></p>
                <ul class="small">
                    <li>Projector not learning properly</li>
                    <li>Decoder LoRA rank too low</li>
                </ul>
                <p style="margin-top: 1em;"><strong>Solutions:</strong></p>
                <ul class="small">
                    <li>Increase decoder rank (r=128)</li>
                    <li>Lower learning rate</li>
                </ul>
            </section>

            <!-- Training Issue 4 -->
            <section>
                <h2>Issue 4: Out of Memory</h2>
                <p><strong>Symptoms:</strong></p>
                <ul class="small">
                    <li>CUDA out of memory errors</li>
                    <li>Training crashes</li>
                </ul>
                <p style="margin-top: 1em;"><strong>Solutions:</strong></p>
                <ul class="small">
                    <li>Reduce batch size (4 instead of 8)</li>
                    <li>Increase gradient accumulation to keep effective batch size</li>
                    <li>Use gradient checkpointing</li>
                    <li>Limit max audio length (25s instead of 30s)</li>
                </ul>
            </section>

            <!-- Inference Issues -->
            <section>
                <h2>Inference Issues</h2>
                <div class="two-column">
                    <div>
                        <h4 class="highlight">Slow inference</h4>
                        <ul class="small">
                            <li>Use Flash Attention 2</li>
                            <li>Batch multiple samples</li>
                            <li>Mixed precision (bf16)</li>
                            <li>Model quantization</li>
                        </ul>
                    </div>
                    <div>
                        <h4 class="highlight">Poor quality on specific audio</h4>
                        <ul class="small">
                            <li>Collect more data</li>
                            <li>Analyze error patterns</li>
                            <li>Domain-specific fine-tuning</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- Key Takeaways -->
            <section>
                <h2>Key Takeaways</h2>
                <ul>
                    <li>WER measures ASR quality (lower is better)</li>
                    <li>Tiny Audio targets ~12% WER</li>
                    <li>Text normalization ensures fair comparison</li>
                    <li>Common issues: flat loss, overfitting, gibberish, OOM</li>
                    <li>Solutions: adjust LR, batch size, LoRA rank, data</li>
                    <li>Context matters - different domains have different WER expectations</li>
                </ul>
            </section>

            <!-- Today's Workshop -->
            <section>
                <h2>Today's Hands-On Workshop</h2>
                <p>What you'll do in the next 40 minutes:</p>
                <ul>
                    <li><strong>Exercise 1:</strong> Run evaluation on your trained model</li>
                    <li><strong>Exercise 2:</strong> Analyze error patterns and predictions</li>
                    <li><strong>Exercise 3:</strong> Debug training issues (if any)</li>
                    <li><strong>Exercise 4:</strong> Calculate and interpret WER metrics</li>
                </ul>
                <p style="margin-top: 1.5em; color: #42affa;">
                    By the end, you'll know exactly how well your model performs!
                </p>
            </section>

            <!-- Thank You -->
            <section>
                <h2>Questions?</h2>
                <p>Let's move to the hands-on workshop!</p>
                <p class="small">Press <code>Esc</code> for slide overview • <code>S</code> for speaker notes</p>
            </section>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/highlight/highlight.js"></script>
    <script>
        Reveal.initialize({
            hash: true,
            plugins: [ RevealNotes, RevealHighlight ],
            transition: 'slide',
            backgroundTransition: 'fade'
        });
    </script>
</body>
</html>
