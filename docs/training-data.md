# Training Data Examples

This document shows examples of the different training data formats used for model training.

## Example 1: Transcription Task

**Audio**: A person saying "The weather is beautiful today."

**Training Format**:

```
User: <audio><audio><audio>... Transcribe:
Assistant: the weather is beautiful today.
```

**Source Datasets**: LoquaciousSet, AMI

______________________________________________________________________

## Example 2: Answer Extraction Task

**Audio**: Someone asking "What is the capital of France?"

**Training Format**:

```
User: <audio><audio><audio>...
Assistant: paris
```

**Source Dataset**: HeySQuAD

______________________________________________________________________

## Example 3: Conversational Response

**Audio**: A happy woman saying "I got the job!"

**Training Format**:

```
User: <audio><audio><audio>...
Assistant: That's wonderful news! Congratulations on your new position!
```

**Source Dataset**: mazesmazes/sift-audio-2

**Note**: No instruction prompt - model learns to respond conversationally to what it hears.

______________________________________________________________________

## Example 4: Empathetic Response

**Audio**: A sad man saying "I lost my keys again."

**Training Format**:

```
User: <audio><audio><audio>...
Assistant: Oh no, that's frustrating! Have you checked your coat pockets?
```

**Source Dataset**: mazesmazes/sift-audio-2

**Note**: Model learns to respond with awareness of speaker's emotional tone.

______________________________________________________________________

## Example 5: Audio Description

**Audio**: An angry young adult male saying "Leave me alone!"

**Training Format**:

```
User: <audio><audio><audio>... Describe all information you can hear:
Assistant: Sounds like an angry young male saying "leave me alone" in a harsh, frustrated voice.
```

**Source Dataset**: mazesmazes/sift-audio-2

**Note**: This mode includes an instruction prompt, teaching the model to describe audio on demand.

______________________________________________________________________

## How SIFT Responses Are Generated

Examples 3-5 use the SIFT (Self-Instruction Fine-Tuning) approach. The assistant responses are **not** generated by a model listening to audio. Instead:

1. **Paralinguistic metadata is extracted** from source datasets (CREMA-D, RAVDESS, MELD, etc.) including: emotion, gender, age, speaking pace, volume, and accent.

1. **Metadata is formatted as structured context** for an LLM:

   ```
   <audio><meta>gender: male, emotion: sad</meta><text>i lost my keys again.</text></audio>
   ```

1. **An LLM (SmolLM3-3B) generates responses** using mode-specific system prompts:

   - **sift_ssp** (Example 4): "Respond empathetically, being aware of their tone and emotion"
   - **sit_ssp** (Example 5): "Describe audio starting with 'Sounds like', include emotion, gender, quoted speech"

1. **During training**, the real audio is paired with these generated responses, teaching the model to produce similar outputs from actual audio embeddings.

This approach allows generating large-scale training data for paralinguistic understanding without requiring expensive human annotation or a pre-existing audio-understanding model.

______________________________________________________________________

## Example 6: Speech-to-Speech (S2S)

**Audio**: A person saying "The weather is beautiful today."

**Training Format**:

```
User: <audio><audio><audio>...
Assistant: the weather is beautiful today.
```

**Codec Targets** (parallel supervision for Audio Head):

```
codec_targets: [8, T]  # 8 Mimi codebooks Ã— T frames

Codebook 0 (semantic):  [1842, 1203, 892, 1456, ...]   # AR decoder target
Codebook 1 (acoustic):  [456, 1823, 234, 901, ...]    # Depformer target
Codebook 2 (acoustic):  [78, 1234, 567, 2001, ...]    # Depformer target
...
Codebook 7 (acoustic):  [234, 890, 1567, 432, ...]    # Depformer target
```

**Source Dataset**: mazesmazes/libritts-mimi

**Note**: S2S training uses two parallel objectives:

1. **Text loss**: LLM learns to generate transcription from audio embeddings
1. **Codec loss**: Audio Head learns to generate Mimi codes from LLM hidden states

The codec targets are pre-computed using the Mimi encoder and stored alongside the audio.

______________________________________________________________________
