# @package _global_
# MoE projector (shared expert + sparse routed experts)

model:
  audio_model_id: zai-org/GLM-ASR-Nano-2512
  text_model_id: Qwen/Qwen3-1.7B
  projector_type: moe

  # LoRA configuration
  use_lora: true
  lora_r: 64
  lora_alpha: 32
  lora_dropout: 0.0
  lora_target_modules:
    - v_proj
    - q_proj

training:
  hub_model_id: "mazesmazes/tiny-audio"
  weight_decay: 0.01
  warmup_steps: 1000
  per_device_train_batch_size: 14
  per_device_eval_batch_size: 14
  gradient_accumulation_steps: 2
  num_train_epochs: 1
  save_steps: 500
  eval_steps: 1000