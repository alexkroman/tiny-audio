# @package _global_
# Stage 3: Joint fine-tuning of projector + LoRA adapter
# Unfreezes projector to jointly optimize both components
#
# Usage:
#   poetry run python scripts/train.py +experiments=mlp_fine_tune
#
# Requires a pretrained Stage 2 model (projector + LoRA trained)

model:
  projector_type: mlp
  projector_pool_stride: 4
  audio_model_id: zai-org/GLM-ASR-Nano-2512
  text_model_id: Qwen/Qwen3-0.6B
  pretrained_model_path: "mazesmazes/tiny-audio"  # Stage 2 checkpoint

  # LLM LoRA config (same as Stage 2)
  use_lora: true
  lora_rank: 64
  lora_alpha: 256
  lora_dropout: 0.05
  lora_target_modules:
    - q_proj
    - v_proj
  freeze_projector: false  # Jointly train projector + LoRA

training:
  hub_model_id: "mazesmazes/tiny-audio"
  learning_rate: 5e-5  # Lower LR for fine-tuning
  seed: 878  # Different seed for data ordering
  per_device_train_batch_size: 14
  per_device_eval_batch_size: 14
  gradient_accumulation_steps: 2
  num_train_epochs: 1
  save_steps: 1000
  eval_steps: 1000

  # SpecAugment for Stage 3 fine-tuning
  use_specaugment: true
  mask_time_prob: 0.1  # 10% of time steps masked
  mask_time_length: 15  # Max length of time mask
  mask_time_min_masks: 2  # Min number of time masks
  mask_feature_prob: 0.05  # 5% of frequency bins masked
  mask_feature_length: 10  # Max length of frequency mask
  mask_feature_min_masks: 2  # Min number of frequency masks
