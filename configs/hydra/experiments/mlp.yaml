# @package _global_

# Simple 2-layer MLP projector (Voxtral-style)
# No downsampling - preserves sequence length
defaults:
  - override /model: large
  - override /data: loquacious_medium
  - override /training: production

model:
  decoder_model_name: HuggingFaceTB/SmolLM3-3B
  encoder_model_name: openai/whisper-large-v3-turbo

  projector_type: mlp

training:
  # Hub configuration
  push_to_hub: true
  hub_model_id: "mazesmazes/tiny-audio-mlp"
  hub_strategy: "every_save"
  hub_private_repo: false

  per_device_train_batch_size: 12
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 2  # Effective batch = 24
  eval_accumulation_steps: 1

  adam_beta1: 0.9
  adam_beta2: 0.95

  learning_rate: 1.0e-4

  lr_scheduler_type: "cosine"
  warmup_steps: 500

  weight_decay: 0.01
  max_grad_norm: 1.0
  projector_dropout: 0.0
  use_specaugment: true
  label_smoothing: 0.0

  data:
    max_audio_duration_seconds: 15.0

  # Memory optimization
  torch_compile: false

  # Data loading optimization
  dataloader_num_workers: 20
  dataloader_prefetch_factor: 8
  dataloader_pin_memory: true
  dataloader_persistent_workers: true
  dataloader_drop_last: true

  group_by_length: true

  save_steps: 1000
  eval_steps: 2000
  logging_steps: 50

  num_train_epochs: 1
  max_steps: -1

  seed: 44
  ignore_data_skip: true

# MLP projector works with streaming
data:
  use_streaming: true

# Early stopping configuration
early_stopping:
  patience: null
  threshold: 0.0
