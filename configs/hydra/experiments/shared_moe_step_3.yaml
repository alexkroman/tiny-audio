# @package _global_

# Stage 3 training - surgical focus on remaining gaps (CommonVoice, Peoples, AMI)
# Builds on Stage 2 with lower LR and aggressive dataset weighting
defaults:
  - override /model: large
  - override /data: combined_stage3
  - override /training: production

model:
  pretrained_model_path: "mazesmazes/tiny-audio-moe-shared"
  decoder_model_name: HuggingFaceTB/SmolLM3-3B
  encoder_model_name: openai/whisper-large-v3-turbo
  projector_type: shared_moe

training:
  push_to_hub: true
  hub_model_id: "mazesmazes/tiny-audio-moe-shared"
  hub_strategy: "every_save"
  hub_private_repo: false

  per_device_train_batch_size: 8
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 4

  adam_beta1: 0.9
  adam_beta2: 0.999

  learning_rate: 2.0e-5
  lr_scheduler_type: "cosine"
  warmup_steps: 100

  weight_decay: 0.1
  max_grad_norm: 0.5
  use_specaugment: true
  label_smoothing: 0.1

  torch_compile: true
  max_steps: 15000

  seed: 345
  ignore_data_skip: true

data:
  max_audio_duration_seconds: 25.0

early_stopping:
  patience: null
  threshold: 0.0
