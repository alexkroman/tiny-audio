# @package _global_

defaults:
  - override /model: large
  - override /data: loquacious_large
  - override /training: production

# Model configuration - the freezing logic is hard-coded in asr_modeling.py
model:
  decoder_model_name: HuggingFaceTB/SmolLM3-3B  # Base model without chat templates
  encoder_model_name: facebook/hubert-xlarge-ls960-ft

  # Projector configuration
  projector_hidden_dim: 2048
  audio_downsample_rate: 5  # Concatenate every 5 frames for downsampling

training:

  resume_from_checkpoint: /workspace/outputs/2025-10-14/22-36-14/outputs/production_model/checkpoint-20000

  learning_rate: 3.0e-4  # Increased from 1e-4 for better convergence
  warmup_steps: 100     # Increased from 500 for smoother ramp-up
  lr_scheduler_type: cosine

  weight_decay: 0.01     # Small weight decay to prevent overfitting
  max_grad_norm: 2.0     # Increased to allow larger updates

  # Evaluation and checkpointing
  eval_steps: 500
  save_steps: 500
  logging_steps: 25

  # Training duration
  max_steps: 40000

  # Reproducibility
  seed: 43
  ignore_data_skip: true

early_stopping:
  patience: 10           # Reduced patience - stop faster if not improving
  threshold: 0.0005      # Tighter threshold

# Resume from stage1 checkpoint
