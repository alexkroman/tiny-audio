# @package _global_

# Stage 2 configuration - continues from stage1 with different seed
defaults:
  - override /model: large
  - override /data: loquacious_large
  - override /training: production

model:

  pretrained_model_path: "mazesmazes/tiny-audio"  # Load projector from here
  decoder_model_name: HuggingFaceTB/SmolLM3-3B
  encoder_model_name: facebook/hubert-xlarge-ls960-ft

  # Projector configuration
  audio_downsample_rate: 5  # Concatenate every 5 frames for downsampling
  projector_hidden_dim: 8192  # Increased for better capacity

training:
  per_device_train_batch_size: 12
  per_device_eval_batch_size: 12
  gradient_accumulation_steps: 3
  eval_accumulation_steps: 3

  # Hub configuration
  push_to_hub: true
  hub_model_id: "mazesmazes/tiny-audio"  # Change to your HF username
  hub_strategy: "checkpoint"  # Push every checkpoint
  hub_private_repo: false  # Keep it private

  # Learning rate and scheduler - constant LR to break through plateau
  learning_rate: 5e-4  # Increased from 5e-6 to enable meaningful updates
  warmup_steps: 100  # Gradual warmup to target LR
  lr_scheduler_type: cosine  # No decay - maintain learning capacity

  # Optimization - more responsive Adam for plateau
  weight_decay: 0.01
  max_grad_norm: 1.5  # Increased from 1.0 for less aggressive clipping
  adam_beta2: 0.95  # Reduced from 0.999 for faster adaptation to current gradients
  adam_epsilon: 1.0e-6  # Reduced from 1e-8 for slightly larger updates to low-variance parameters

  # Evaluation and checkpointing - more frequent monitoring
  eval_steps: 500  # Increased frequency to monitor plateau breaking
  save_steps: 500
  logging_steps: 25

  # Training duration - extended for stage 2
  max_steps: 10000

  # Reproducibility - different seed for stage 2
  seed: 49  # Changed from 42 for different initialization
  ignore_data_skip: true
