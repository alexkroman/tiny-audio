# @package _global_

# Multi-task training configuration - All tasks combined
# Includes: transcription, continuation, description, and emotion recognition
# Based on stage1 configuration with multi-task data
defaults:
  - override /model: large
  - override /data: combined
  - override /training: production

model:
  # Load the pretrained model from Hub (this loads the projector weights)
  # pretrained_model_path: "mazesmazes/tiny-audio-swiglu"

  decoder_model_name: HuggingFaceTB/SmolLM3-3B
  encoder_model_name: openai/whisper-large-v3-turbo

  # Projector type: "moe" (Mixture of Experts) or "swiglu" (simple SwiGLU)
  projector_type: swiglu

training:
  # Resume from checkpoint
  # resume_from_checkpoint: "/workspace/outputs/2025-11-26/18-26-08/outputs/production_model/checkpoint-13000"

  # Hub configuration
  push_to_hub: true
  hub_model_id: "mazesmazes/tiny-audio-swiglu"
  hub_strategy: "every_save"  # Push to root on every save (includes tokenizer)
  hub_private_repo: false

  per_device_train_batch_size: 8
  per_device_eval_batch_size: 24
  gradient_accumulation_steps: 3
  eval_accumulation_steps: 3

  adam_beta1: 0.9
  adam_beta2: 0.999

  learning_rate: 2.0e-4

  lr_scheduler_type: "cosine"
  warmup_steps: 1000  # Critical for MoE start

  weight_decay: 0.01
  max_grad_norm: 1.0
  projector_dropout: 0.1
  use_specaugment: true
  label_smoothing: 0.1

  # Memory optimization
  torch_compile: true

  max_steps: 15000

  # Reproducibility
  seed: 42
  ignore_data_skip: false

# Early stopping configuration
early_stopping:
  patience: null
  threshold: 0.0