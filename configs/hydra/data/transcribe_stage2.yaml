# Stage 2 transcription dataset - Rebalanced for better epoch coverage
# 50% GigaSpeech, 30% LoquaciousSet, 10% Earnings22, 10% AMI
# Target: ~1.4M samples over 50k steps (batch_size=14, grad_accum=2)
# Expected epochs: GigaSpeech (0.07), LoquaciousSet (0.84), Earnings22 (11.7), AMI (2.8)
datasets:
  # Transcription task - GigaSpeech (50% - diverse long-form)
  - path: fixie-ai/gigaspeech
    name: xl-empty-audio-removed
    audio_column: audio
    text_column: text
    task: transcribe
    train_splits:
      - train
    eval_splits:
      - train
    sampling_weight: 0.50

  # Transcription task - LoquaciousSet (30% - general diverse speech)
  - path: speechbrain/LoquaciousSet
    name: large
    audio_column: wav
    text_column: text
    task: transcribe
    train_splits:
      - train
    eval_splits:
      - dev
    sampling_weight: 0.30

  # Transcription task - Earnings22 (10% - financial domain)
  - path: distil-whisper/earnings22
    name: chunked
    audio_column: audio
    text_column: text
    task: transcribe
    train_splits:
      - test
    eval_splits:
      - test
    sampling_weight: 0.10

  # Transcription task - AMI (10% - meeting conversations)
  - path: TakalaWang/AMI_ASR
    audio_column: audio
    text_column: text
    task: transcribe
    train_splits:
      - train
    eval_splits:
      - test
    sampling_weight: 0.10

# Audio processing
sample_rate: 16000

# SpecAugment parameters (moderate masking for diverse data)
mask_time_prob: 0.05
mask_time_length: 10
mask_feature_prob: 0.0
mask_feature_length: 10

# Processing options
dataset_cache_dir: ${hydra:runtime.cwd}/datasets_cache
max_train_samples: null
max_eval_samples: 500  # Keep eval manageable
use_streaming: true
