# Large model configuration for SLAM-ASR
decoder_model_name: HuggingFaceTB/SmolLM3-3B  
encoder_model_name: facebook/hubert-xlarge-ls960-ft  # HuBERT encoder

# Projector configuration per SLAM-ASR paper
projector_hidden_dim: 2048
audio_downsample_rate: 5

# System prompt for transcription task
system_prompt: "/system_override"

# Attention implementation - auto-detected by training script
# Will use flash_attention_2 on compatible GPUs (compute capability >= 8.0)
# Falls back to sdpa (PyTorch scaled dot-product attention) otherwise
