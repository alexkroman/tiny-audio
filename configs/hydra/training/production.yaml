# Production training configuration
output_dir: ./outputs/production_model

optim: adamw_torch_fused
adam_beta1: 0.9
adam_beta2: 0.999  
adam_epsilon: 1e-8 

# Logging
logging_steps: 25
logging_first_step: true
logging_nan_inf_filter: false
report_to: wandb

# Evaluation
eval_strategy: steps
eval_steps: 1000

# Saving
save_strategy: steps
save_steps: 500
save_total_limit: 5
load_best_model_at_end: false
metric_for_best_model: eval_loss
greater_is_better: false

# Hub
push_to_hub: true
hub_model_id: "mazesmazes/tiny-audio"  # Change this to your repo
hub_strategy: all_checkpoints  # Push all checkpoints + final model
hub_token: null  # Will use HF_TOKEN env var if not set
hub_private_repo: false

# Performance
fp16: false
bf16: true
tf32: true  # Enable TF32 for A40 (2x matmul speedup)
max_grad_norm: 1.0 
torch_compile: false
# torch_compile_mode: reduce-overhead

# Model dtype - use bfloat16 for production with flash attention
model_dtype: bfloat16
attn_implementation: flash_attention_2

dataloader_num_workers: 0
# dataloader_persistent_workers: true
# dataloader_pin_memory: true

# Misc
seed: 42
save_safetensors: true
disable_tqdm: false
log_level: info

# Required by HF Trainer
remove_unused_columns: false
label_names:
  - labels