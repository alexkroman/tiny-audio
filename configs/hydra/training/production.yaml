# Production training configuration
output_dir: ./outputs/production_model

# Training configuration
per_device_train_batch_size: 18
per_device_eval_batch_size: 18
gradient_accumulation_steps: 1
eval_accumulation_steps: 1

optim: adamw_torch_fused
adam_beta1: 0.9
adam_beta2: 0.999  
adam_epsilon: 1e-8 

# Logging
logging_steps: 25
logging_first_step: true
logging_nan_inf_filter: false
report_to: tensorboard

# Evaluation
eval_strategy: steps
eval_steps: 500

# Saving
save_strategy: steps
save_steps: 500
save_total_limit: 10
load_best_model_at_end: true
metric_for_best_model: eval_loss
greater_is_better: false

# Hub
push_to_hub: true
hub_model_id: "mazesmazes/tiny-audio"  # Change this to your repo
hub_strategy: checkpoint  # Push every checkpoint
hub_token: null  # Will use HF_TOKEN env var if not set
hub_private_repo: false

# Performance
fp16: false
bf16: true
tf32: true  # Enable TF32 for A40 (2x matmul speedup)
dataloader_pin_memory: true
max_grad_norm: 1.0  # Increased from 1.0 - less aggressive clipping for projector training
torch_compile: true
# torch_compile_mode: reduce-overhead

# Model dtype - use bfloat16 for production with flash attention
model_dtype: bfloat16
attn_implementation: flash_attention_2

# Streaming compatibility settings
dataloader_num_workers: 8 # Increased for better throughput (was 2)
dataloader_persistent_workers: true
dataloader_prefetch_factor: 4  # Reduced to limit memory with more workers

# Misc
seed: 42
save_safetensors: true
disable_tqdm: false
log_level: info

# Required by HF Trainer
remove_unused_columns: false
label_names:
  - labels